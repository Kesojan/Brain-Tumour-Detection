# -*- coding: utf-8 -*-
"""HeartDiseaseDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15L7MRYIZzqufMA5Ud3HgkEwnMOqEP-zg
"""

# Built by: Kesojan P
# Date: 05/02/2020
# Description: Built and trained a Machine Learning model which can
#              detect whether a patient has Cardiovascular Disease 
#              (Heart Disease) or not with approx 87.9% accuracy
# Tools: Python, NumPy, pandas, Seaborn, SciKit, matplotlib

#import libraries
import numpy as np
import pandas as pd
import seaborn as sns

#load data
from google.colab import files
uploaded= files.upload()

#store data into var
df =pd.read_csv('cardio_train.csv', sep =';')

#print the data
df.head(7)

#Retrieve the shape of the data
df.shape

#Count the empty values in data set
df.isna().sum()

#Another method to check for missing/null values
df.isnull().values.any()

#View some basic stats
df.describe()

#Get a count of the number of patients with cardiovasuclar disease
df['cardio'].value_counts()

#Visualize the data 
sns.countplot(df['cardio'])

#Look at the number of people with a cardiovasuclar disease that exceed number of people wihtoout cardiovascular disease
#create years coloumn

df['years']=(df['age']/365).round(0)
df ['years']= pd.to_numeric( df['years'], downcast='integer')

#Visualize data
sns.countplot(x='years', hue='cardio', data=df, palette='colorblind', edgecolor=sns.color_palette('dark',n_colors=1))

# Get the correlation of the coloumns
df.corr()

#Visualize data to gain insight on correlations and trends
import matplotlib.pyplot as plt

#Output heatmap to see the dataset
plt.figure(figsize=(7,7))
sns.heatmap(df.corr(), annot=True, fmt='.0%')

# Remove or drop the years coloumn
df=df.drop('years', axis=1)

#Remove the id coloumn as it will deter accuracy due when training model
df=df.drop('id', axis=1)

#Split the data into feature data and target data
X= df.iloc[:, :-1].values
Y= df.iloc[:,-1]. values

#Split the data again, into 75% trainig and 25% testing
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test=train_test_split(X,Y, test_size=0.25,random_state=1)

#Feature scaling
#Scale the values in data to be values between 0 and 1 inclusive
from sklearn.preprocessing import StandardScaler

sc =StandardScaler()
X_train = sc.fit_transform(X_train)
X_test =sc.transform(X_test)

#import ML training method Random Forest
from sklearn.ensemble import RandomForestClassifier

#Use random forest classifier to train data for accuracy
forest=RandomForestClassifier(n_estimators =10, criterion ='entropy', random_state=1)
forest.fit(X_train, Y_train)

#Test the models accuracy on the training data set
model= forest
model.score(X_train, Y_train)

#Test the models accuracy on test data set
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(Y_test, model.predict(X_test))

TN =cm[0][0]
TP =cm[1][1]
FN =cm[1][0]
FP =cm[0][1]

#Print the confusion matrix data
print(cm)

#Print the model accurcay on the test data (75%)
print('Model Test Accuracy= {}'.format((TP+TN)/ (TP +TN+FN+FP)))

